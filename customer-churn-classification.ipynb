{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7674734,"sourceType":"datasetVersion","datasetId":4476823}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/noahnguyen08/customer-churn-classification-and-network-analysis?scriptVersionId=164925678\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:23:21.354989Z","iopub.execute_input":"2024-02-29T23:23:21.355484Z","iopub.status.idle":"2024-02-29T23:23:21.886133Z","shell.execute_reply.started":"2024-02-29T23:23:21.355439Z","shell.execute_reply":"2024-02-29T23:23:21.884823Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/customer-churn-data-infinite-investment-systems/account_data_info.csv\n/kaggle/input/customer-churn-data-infinite-investment-systems/history.csv\n/kaggle/input/customer-churn-data-infinite-investment-systems/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install kmodes\n!pip install networkx\n!pip install prince","metadata":{"execution":{"iopub.status.busy":"2024-02-29T23:23:21.888837Z","iopub.execute_input":"2024-02-29T23:23:21.889937Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: kmodes in /opt/conda/lib/python3.10/site-packages (0.12.2)\nRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.10/site-packages (from kmodes) (1.24.4)\nRequirement already satisfied: scikit-learn>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from kmodes) (1.2.2)\nRequirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from kmodes) (1.11.4)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from kmodes) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.0->kmodes) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport prince                 # MCA\n\nimport datetime\nimport category_encoders as ce\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom kmodes.kmodes import KModes\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.metrics import roc_curve, auc\n\nfrom PIL import Image\n\n# from kneed import KneeLocator\n%matplotlib inline","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6597,"status":"ok","timestamp":1708567609753,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"i7pjs4elmOz4","outputId":"a6db6921-26b7-4c8b-9762-2d73bce093b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/customer-churn-data-infinite-investment-systems/history.csv\")\ntest = pd.read_csv(\"/kaggle/input/customer-churn-data-infinite-investment-systems/test.csv\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35617,"status":"ok","timestamp":1708567645367,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"lQp053TamoqN","outputId":"0a9e7aeb-102c-4abb-c674-00b4a10e6199","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_info = pd.read_csv(\"/kaggle/input/customer-churn-data-infinite-investment-systems/account_data_info.csv\", encoding = \"latin-1\")","metadata":{"executionInfo":{"elapsed":797,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"AQLZ5f_gau6_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = df[:]        # make a copy for customer churn data\ntest_copy = test[:]    # make a copy for test data","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"JzyrB90COn3C","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_info_copy = data_info[:]","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"da0DZe7t0fzF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing","metadata":{}},{"cell_type":"markdown","source":"Description of each feature","metadata":{"id":"yFJ6z7t2bh4x"}},{"cell_type":"code","source":"data_info_copy.head(10)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"AxXAcujjadfL","outputId":"314e781d-e8c0-4e76-88ec-df955c36ecf5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dealing with missing values","metadata":{"id":"RpvAtl0h24zT"}},{"cell_type":"markdown","source":"There are missing data in the description of some features. We'd proceed imputing these missing values with an empty string.","metadata":{"id":"-oSB9EkO0CEu"}},{"cell_type":"code","source":"data_info_copy['Description'] = data_info_copy['Description'].fillna(' ')   # imputing missing data with empty string\ndata_info_copy['Field Values'] = data_info_copy['Field Values'].fillna(' ')","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"bzoTUkky2iEd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_info_copy['Columns'][1]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"x3kHIZqd1Or3","outputId":"9968ad75-cc3c-4230-bd4d-017e8376d5ef","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_info_copy.head(5)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"OlPcD646kVlR","outputId":"24a0730f-819b-4158-d45f-e4eb780229c2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)    # show all columns","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"PVcVpMjPOwjM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy.columns[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"54dHy9nQg3Xg","outputId":"ac81d1bd-985a-48b3-8a11-2f73d6a9536d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that the number of columns in the data description and the customer data is not the same. We'd check what are the columns that don't exist in the customer data.","metadata":{"id":"G74dsu0nfFRd"}},{"cell_type":"code","source":"data_info_lst = []\ncustomer_data_lst = []\nfor val in range(len(data_info_copy['Columns'])):\n  data_info_lst.append(data_info_copy['Columns'][val])\n\nfor col in range(len(df_copy.columns)):\n  customer_data_lst.append(df_copy.columns[col])","metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1708567646154,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"plLf9to7fZ_I","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_info_lst)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1708567646155,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"0xtPhYKPgZBL","outputId":"4ac1021b-864d-4cd4-d79b-cb60f3782549","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(customer_data_lst)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1708567646155,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"3aZJGl9pgcPX","outputId":"5a67b42a-df36-40ab-878c-3e62f7ecb904","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_DNE = []\nfor i in range(len(data_info_lst)):\n  if data_info_lst[i] not in customer_data_lst:\n    info_DNE.append(data_info_lst[i])","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1708567646437,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"-3gwspC2hZ89","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_DNE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1708567646437,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"SXQWdtPQiBnz","outputId":"702dbc4d-f78d-4e4f-8300-1416c3f044b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are the columns that didn't appear on the customer data.","metadata":{"id":"BCNt8xzZieqf"}},{"cell_type":"code","source":"df_copy['country_code'].isnull().value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708567646437,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"PySgNemYkm3G","outputId":"c2a376ae-b468-45a5-c181-45d1c3f9aa65","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy['currency_code'].isnull().value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1708567646437,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"L1j_ruD0liSn","outputId":"bf19d1f3-5e72-4c92-9c06-52244e74b36b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`country_code` and `currency_code` have the same purpose, let's say your currency code is CAD, it's likely that you are from Canada, thus the `country_code` may not contribute any useful information. Thus we'd drop this column.","metadata":{"id":"ztp043Iznpq8"}},{"cell_type":"code","source":"df_copy = df_copy.drop('country_code', axis=1)","metadata":{"executionInfo":{"elapsed":985,"status":"ok","timestamp":1708567647420,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"5yePlH2DnomL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy['cashflows_custody_fee'].isnull().value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1708567647426,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"io_Q6m0JoCMf","outputId":"39b99bb7-15d6-4bbf-d95e-7ba2cb0d45c7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As all the records in `cashflows_custody_fee` are missing, we'd just drop this column as well.","metadata":{"id":"k-2h0IQ_oy9j"}},{"cell_type":"code","source":"df_copy = df_copy.drop('cashflows_custody_fee', axis=1)","metadata":{"executionInfo":{"elapsed":671,"status":"ok","timestamp":1708567648074,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"H5VmVYO5ow40","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy['esir_number'].isnull().value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708567648075,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"LirR-xvno6Cf","outputId":"e3f21b67-df02-4f5a-a913-7624ec854e39","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = df_copy.drop('esir_number', axis=1)","metadata":{"executionInfo":{"elapsed":585,"status":"ok","timestamp":1708567649017,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"ANmTbFDepzSs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def missing_data_per_col(data):\n  '''\n    Show the number of missing values\n    in each column\n\n    input: data\n  '''\n  null_count = []\n  missing_data = {}\n  for col in data.columns:\n    missing_data[col] = [data[col].isna().sum()]\n  missing_df = pd.DataFrame(missing_data)\n  missing_df = missing_df.rename(columns = {0: \"Number of missing values\"})\n  return missing_df\n","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1708567649017,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"NzudsMN8qLYF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_data_df = missing_data_per_col(df_copy)","metadata":{"executionInfo":{"elapsed":9194,"status":"ok","timestamp":1708567658210,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"6N-fOsglt9_p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_data_df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1708567658617,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"PA3Ktasgt-oY","outputId":"703a4404-32d9-43ed-b009-050971d2f3a3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(missing_data_df.columns)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1708567658617,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"EfkAKuuIMZka","outputId":"8f9abad2-69b8-4daf-902f-349d418f52b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyzing the contribution of each feature","metadata":{"id":"cpn7hCvQIlxD"}},{"cell_type":"markdown","source":"#### Categorical features that have too many categories -> K-modes clustering","metadata":{"id":"H-KAskJ4ZfLS"}},{"cell_type":"code","source":"print(\"Number of categories in class_id: \", len(df_copy['class_id'].unique()))\nprint(\"Number of categories in terminal_code: \", len(df_copy['terminal_code'].unique()))\nprint(\"Number of categories in sss_agent: \", len(df_copy['sss_agent'].unique()))\nprint(\"Number of categories in deceased_fair_market_value: \", len(df_copy['deceased_fair_market_value'].unique()))\nprint(\"Number of categories in risk_tolerance: \", len(df_copy['risk_tolerance'].unique()))\nprint(\"Number of categories in investment_objective: \", len(df_copy['investment_objective'].unique()))\nprint(\"Number of categories in last_maintenance_user: \", len(df_copy['last_maintenance_user'].unique()))\nprint(\"Number of categories in special_fee_code: \", len(df_copy['special_fee_code'].unique()))\nprint(\"Number of categories in retail_last_maintenance_user: \", len(df_copy['retail_last_maintenance_user'].unique()))\nprint(len(df_copy['retail_last_maintenance_user'].unique()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1882,"status":"ok","timestamp":1708567660497,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"kIV4LFhwYoGt","outputId":"b7e1f0bb-9c67-4978-ac93-eb059ba839ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop `rep_commission_rate`, `dup_trip_quad_code`, `portfolio_cost_method`, `portfolio_name_address_option` columns as they only have one single value.","metadata":{"id":"KhVSA5c_bNm1"}},{"cell_type":"code","source":"# df_copy = df_copy.drop('rep_commission_rate', axis=1)\n# df_copy = df_copy.drop('dup_trip_quad_code', axis=1)\n# df_copy = df_copy.drop('portfolio_cost_method', axis=1)\n# df_copy = df_copy.drop('portfolio_name_address_option', axis=1)\n# df_copy = df_copy.drop('portfolio_summary_option', axis=1)\n# df_copy = df_copy.drop('interactive_portfolio_code', axis=1)\n# df_copy = df_copy.drop('non_plan_book_value_flag', axis=1)","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1708567660497,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"lFezRH3KbWjb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"elapsed":392,"status":"ok","timestamp":1708567660887,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"L9ABe562al-D","outputId":"969c15d9-ac4a-4f12-f73f-e3db59ab83a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy_time_based_cols = df_copy[['inception_date', 'last_update_date', 'last_maintenance_time', 'rrif_original_date', 'plan_effective_date', 'plan_end_date', 'last_trade_date', 'inserted_at', 'updated_at', 'retail_last_maintenance_time']]\n# for time-based columns, create dummy variables -> year, month, day and convert them to int dtype\n\n\ndf_copy_bool_based_cols = df_copy[['is_registered', 'is_active', 'net_of_fees', 'fee_paid_separately', 'custody_fee_withdrawal', 'is_fee_exempt', 'include_client_consolidation', 'use_client_address',\n                                   'is_spousal', 'is_arp_locked', 'is_midwest_clearing_account', 'use_hand_delivery', 'use_mail', 'share_name_address_to_issuer', 'shareholder_instructions_received',\n                                   'rrsp_limit_reached', 'is_portfolio_account', 'has_no_min_commission', 'is_tms_eligible', 'is_agent_bbs_participant', 'is_parameters_account', 'is_spousal_transfer',\n                                   'spousal_age_flag', 'has_multiple_name', 'discretionary_trading_authorized', 'receive_general_mailings', 'has_discrete_auth', 'is_non_objecting_beneficial_owner',\n                                   'is_objecting_to_disclose_info', 'consent_to_pay_for_mail', 'consent_to_email_delivery', 'has_received_instruction', 'is_broker_account', 'is_inventory_account',\n                                   'is_gl_account', 'is_control_account', 'is_extract_eligible', 'is_pledged', 'is_resp', 'use_original_date_for_payment_calc', 'is_family_resp', 'is_hrdc_resp',\n                                   'is_plan_grandfathered', 'is_olob', 'visible_in_reports']]\n\n# for bool-based columns, convert to binary (0 and 1)\n\ndf_copy_categorical_based_cols = df_copy[['type_code', 'currency_code', 'class_id', 'debit_code', 'contract_type', 'branch', 'credit_limit_type', 'retail_plan', 'arp_pension_origin', 'language_code', 'dividend_confirm_code',\n                                          'sss_location', 'options_trading_type', 'sss_type', 'sss_agent', 'rep_commission_override', 'interest_dividend_conversion_type', 'guarantee_gtor_type', 'terminal_code',\n                                          'deceased_fair_market_value', 'target_grantor_grantee_flag', 'iso_funds_code', 'shareholder_language', 'special_tag', 'conjunction', 'title', 'function_code', 'tms_settlement_location',\n                                          'portfolio_report_option', 'loan_limit_override', 'mailing_consent', 'risk_tolerance', 'investment_objective', 'last_maintenance_user', 'special_fee_code', 'non_calendar_year_end',\n                                          'number_of_beneficiaries', 'resp_specimen_plan', 'retail_last_maintenance_user']]\n\n# for categorical-based columns, perform clustering (for features that contain a lot of categories)\n\n# the number of clusters will be decided by elbow method (or domain knowledge)","metadata":{"executionInfo":{"elapsed":1835,"status":"ok","timestamp":1708567662703,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"BZRw1Z1iJ22P","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check if there is any missing columns after assigning columns into variables","metadata":{"id":"ZgDwFJgrhhYH"}},{"cell_type":"code","source":"count_missing_col = 0\nfor i in range(len(df_copy_time_based_cols.columns)):\n  if df_copy_time_based_cols.columns[i] not in df_copy.columns:\n    count_missing_col += 1\n\nfor i in range(len(df_copy_bool_based_cols.columns)):\n  if df_copy_bool_based_cols.columns[i] not in df_copy.columns:\n    count_missing_col += 1\n\nfor i in range(len(df_copy_categorical_based_cols.columns)):\n  if df_copy_categorical_based_cols.columns[i] not in df_copy.columns:\n    count_missing_col += 1\nprint(count_missing_col)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708567662704,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"FMfQ2ltngX6z","outputId":"d7fd14fb-b192-4c4a-fb6c-8919d02b971c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check the null values in each feature","metadata":{"id":"_8w-_GfJZEug"}},{"cell_type":"code","source":"nan_count = df_copy.isna().sum()","metadata":{"executionInfo":{"elapsed":16034,"status":"ok","timestamp":1708567678736,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"AyXHzq1qZQBC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nan_count.sort_values(axis=0).value_counts().sort_index()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1708567678736,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"JDrfK29OaxON","outputId":"093d82c6-b9fc-4275-9d96-448503545e0c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that there are 39 columns that contain no missing values. We will perform clustering on those columns","metadata":{"id":"9AR5GaKdeFKQ"}},{"cell_type":"code","source":"no_nan_cols = df_copy[list(nan_count[nan_count == 0].index)]","metadata":{"executionInfo":{"elapsed":418,"status":"ok","timestamp":1708567679142,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"7c8iN8Xwd9x7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have to drop the target variable (label: churn or no churn)","metadata":{"id":"g6_BOZtyhSFq"}},{"cell_type":"code","source":"no_nan_cols = no_nan_cols.drop('label', axis=1)","metadata":{"executionInfo":{"elapsed":319,"status":"ok","timestamp":1708567679459,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"tCMdkwmTgb5D","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_nan_cols = no_nan_cols.drop('id', axis=1)","metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1708567679739,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"1KrsmcjchdDE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_nan_cols","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1708567679745,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"zeQcjtkch6Gh","outputId":"4cdc7d53-e012-47e8-e106-828b344ea7da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_nan_cols_no_time_series = no_nan_cols.drop(['inserted_at', 'updated_at'], axis=1)","metadata":{"executionInfo":{"elapsed":544,"status":"ok","timestamp":1708567680275,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"LNV9NhKrh8Rh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_nan_cols_no_time_series_dummies = pd.get_dummies(no_nan_cols_no_time_series)   # exclude time-series features","metadata":{"executionInfo":{"elapsed":1563,"status":"ok","timestamp":1708567681836,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"j0UgBX8dkmiZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_nan_cols_no_time_series_dummies","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1708567682229,"user":{"displayName":"noah nguyen","userId":"09256501666274157753"},"user_tz":300},"id":"jVJ60wpUjc0u","outputId":"1c1a6493-f3a5-41c6-bf4a-47b8b9335a87","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature analysis\n\nIf some of features contain duplicate information (they convey the same information to our dataset), then it's best to visualize which one they are (how they variate), and decide to drop such features (feature selection).","metadata":{}},{"cell_type":"code","source":"# correlation between features\ncorr_matrix = no_nan_cols_no_time_series_dummies.corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix, annot = True, cmap = 'YlGnBu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above is the Corr[X, Y] where X and Y represent the pair of any 2 features that we examine by correlation heatmap","metadata":{}},{"cell_type":"code","source":"corr_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"High correlation presented: As the correlation heatmap shows, there are a few features that share a high correlation level to another, suggesting that they have a linear relationship with each other\n* Linear relationship means that one feature is the linear transformation of the other, causing multicollinearity (potentially duplicate) information. Thus we would consider drop one of them.\n* We will set a threshold of 0.5 for the correlation level; any pair of features whose level of correlation exceeds the threshold will be examined and excluded by domain knowledge.","metadata":{}},{"cell_type":"code","source":"corr_thresh = 0.5\n\ncorr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if abs(corr_matrix.iloc[i, j] > corr_thresh):\n            corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n\nfor pair in corr_pairs:\n    print(f\"Features '{pair[0]}' and '{pair[1]}' have correlation of: '{pair[2]}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that there are some features that have high correlation with many other features. We will drop such features. (Talk a little bit about the domain knowledge before dropping those features)","metadata":{}},{"cell_type":"code","source":"df_copy = df_copy.drop(['include_client_consolidation', 'use_client_address', 'share_name_address_to_issuer',\n                        'receive_general_mailings', 'is_non_objecting_beneficial_owner', 'is_olob', 'use_mail'], axis=1)\n\nno_nan_cols_no_time_series_dummies = no_nan_cols_no_time_series_dummies.drop(['include_client_consolidation_t', 'include_client_consolidation_f',\n                                                              'use_client_address_t', 'use_client_address_f', 'share_name_address_to_issuer_t',\n                                                              'share_name_address_to_issuer_f', 'receive_general_mailings_t', \n                                                              'receive_general_mailings_f', 'is_non_objecting_beneficial_owner_t',\n                                                              'is_non_objecting_beneficial_owner_f', 'is_olob_t', 'is_olob_f', \n                                                              'use_mail_t', 'use_mail_f'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature importances\n\nLooks like there are still a lot of features. We will continue examine each feature importance to its target variable. We will perform a simple Random Forest model on our non-null data to decide what features to keep, and what to drop.","metadata":{}},{"cell_type":"code","source":"# extract the label used for random forest\nrf_label = df_copy['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_label.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_label = rf_label.apply(lambda x: 1 if x == \"Churn\" else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_nan_data_model = RandomForestClassifier()\nno_nan_data_model.fit(no_nan_cols_no_time_series_dummies, rf_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\nfeature_importances = pd.Series(no_nan_data_model.feature_importances_, index = no_nan_cols_no_time_series_dummies.columns)\nfeature_importances.nlargest(10).plot(kind='barh')\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.title('Top 10 Feature Importances')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This simple Random Forest Classifier did a decent job. As we analyze using our domain knowledge, if a client is **not visible in reports**, it may indicate that the client has been inactive for a long time, leading to the churn decision of that client.","metadata":{}},{"cell_type":"markdown","source":"#### Evaluate the importance of each feature to the target","metadata":{}},{"cell_type":"code","source":"auc_scores = []\nroc_curves = []\n\nfor feature in no_nan_cols_no_time_series_dummies.columns:\n    _feature = no_nan_cols_no_time_series_dummies[[feature]]\n    no_nan_data_model.fit(_feature, rf_label)\n    \n    y_pred = no_nan_data_model.predict_proba(_feature)[:, 1]\n    \n    fpr, tpr, _ = roc_curve(rf_label, y_pred)\n    roc_curves.append((fpr, tpr))\n    \n    auc_score = auc(fpr, tpr)\n    auc_scores.append(auc_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\nfor fpr, tpr in roc_curves:\n    plt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves for each feature')\nplt.legend(no_nan_cols_no_time_series_dummies.columns, loc='lower right', fontsize='x-small')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print AUC score for each feature\nfor i, feature in enumerate(no_nan_cols_no_time_series_dummies.columns):\n    print(f\"Feature '{feature}': AUC = {auc_scores[i]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Question: Should we drop features based solely on AUC score of each feature?","metadata":{}},{"cell_type":"code","source":"#### Rule of thumb for \"good\" AUC score\n\n# Each feature with an AUC score above 0.7 would be considered acceptable, therefore any features with AUC score < 0.7 would be considered for removing.\n\n# df_copy = df_copy.drop(['currency_code', 'is_active', 'net_of_fees', 'fee_paid_separately', 'custody_fee_withdrawal', 'is_fee_exempt', 'use_hand_delivery', 'shareholder_instructions_received', \n#                         'rrsp_limit_reached', 'is_portfolio_account', 'has_no_min_commission', 'is_tms_eligible', 'is_agent_bbs_participant', 'is_parameters_account', 'is_spousal_transfer', \n#                         'spousal_age_flag', 'has_multiple_name', 'is_objecting_to_disclose_info', 'consent_to_pay_for_mail', 'consent_to_email_delivery', 'has_received_instruction', 'is_broker_account', \n#                         'is_inventory_account', 'is_gl_account', 'is_control_account', 'is_extract_eligible'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no_nan_cols_no_time_series_dummies = no_nan_cols_no_time_series_dummies.drop(['currency_code_CAD', 'currency_code_USD', 'is_active_f', 'is_active_t', 'net_of_fees_f',\n#                                                                               'fee_paid_separately_f', 'custody_fee_withdrawal_f', 'is_fee_exempt_f', \n#                                                                               'use_hand_delivery_f', 'use_hand_delivery_t', 'shareholder_instructions_received_f', 'shareholder_instructions_received_t',\n#                                                                               'rrsp_limit_reached_f', 'is_portfolio_account_f', 'has_no_min_commission_f',\n#                                                                               'is_tms_eligible_f', 'is_tms_eligible_t', 'is_agent_bbs_participant_f', 'is_agent_bbs_participant_t', 'is_parameters_account_f',\n#                                                                               'is_spousal_transfer_f', 'is_spousal_transfer_t', 'spousal_age_flag_f', 'has_multiple_name_f', 'has_multiple_name_t',\n#                                                                               'is_objecting_to_disclose_info_f', 'is_objecting_to_disclose_info_t', 'consent_to_pay_for_mail_f', 'consent_to_pay_for_mail_t',\n#                                                                               'consent_to_email_delivery_f', 'consent_to_email_delivery_t', 'has_received_instruction_f', 'has_received_instruction_t',\n#                                                                               'is_broker_account_f', 'is_inventory_account_f', 'is_inventory_account_t', 'is_gl_account_f',\n#                                                                               'is_control_account_f', 'is_extract_eligible_f', 'is_extract_eligible_t'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_nan_cols_no_time_series_dummies","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As clustering on the above data would cause huge time complexity and resources, we have to perform some dimensionality reduction techniques such as PCA to extract new set of features that bring most variance (retain the original information) of our dataset.","metadata":{}},{"cell_type":"markdown","source":"### PCA","metadata":{}},{"cell_type":"code","source":"no_nan_cols_dummies_train = np.array(no_nan_cols_no_time_series_dummies).astype(np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_nan_cols_dummies_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cov_mat = np.cov(no_nan_cols_dummies_train.T)\neigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n\nprint(f\"Eigenvalues: \\n {eigen_vals}\")\nprint(f\"Eigenvalues shape: {eigen_vals.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def explain_variance(eigenvalues):\n    '''\n        Plot how many PCA axes needed to explain most variance\n        from the original feature space of our dataset\n    '''\n    var_explained = []\n    total = sum(eigenvalues)\n    for i in sorted(eigenvalues, reverse=True):\n        var = i / total\n        var_explained.append(var)\n    cummulative_var_explained = np.cumsum(var_explained)\n    plt.bar(range(1, len(eigenvalues)+1), var_explained, alpha=0.5, align='center', label='Individual explained variance ratio')\n    plt.step(range(1, len(eigenvalues)+1), cummulative_var_explained, where='mid', label='Cummulative explained ratio')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal component index')\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explain_variance(eigen_vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, with the number of PCA axes of 10 axes, they already explained almost 100% of the variance of the original data. We will pick 10 as the number of clusters for our SBM disussed later on.","metadata":{}},{"cell_type":"markdown","source":"Since the objective is to classify whether a user churned or didn't churn from the service, so we have to drop the plan_end_date column.","metadata":{}},{"cell_type":"code","source":"df_copy = df_copy.drop('plan_end_date', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have extracted 10 PCA axes as the ideal number of clusters for our clustering algorithm for the non-null dataset.","metadata":{}},{"cell_type":"code","source":"# cluster the data with the number of clusters = 10\noptimal_num_clusters = 10\nkmode_optimal = KModes(n_clusters = optimal_num_clusters, init = 'Cao', n_init = 1, verbose = 1)          # applied K-modes clustering for categorical data\nclusters = kmode_optimal.fit_predict(no_nan_cols_no_time_series_dummies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(clusters)             # 10 unique clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy['Clusters'] = clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if all records per cluster add up to the original number of records\nrec_count = 0\nfor i in range(len(np.unique(clusters))):\n    rec_count = rec_count + len(df_copy.loc[df_copy['Clusters'] == i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rec_count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling missing values\nEach record in our original dataset (containing null values) has been assigned to each of the 10 clusters. We proceed with filling null values by the mode of each column corresponding to each null value and the cluster number.","metadata":{}},{"cell_type":"code","source":"# if the cluster is 0, then loop over every column within each record corresponding to cluster 0. The same thing with other clusters\ndef fill_nan_by_mode(data, cluster_col):\n    '''\n        Fill null values by the mode of each column\n        corresponding to each null value and cluster number\n        \n        data: DataFrame\n        cluster_col: name of the cluster column\n    '''\n    for cluster_num in data[cluster_col].unique():\n        cluster_data = data[data[cluster_col] == cluster_num]\n        \n        # fill null values with mode for each cluster\n        cluster_mode = cluster_data.mode().iloc[0]       # select the first row and calculate the mode for each column in DataFrame, the result is that row containing the mode value of each column\n        data.loc[data[cluster_col] == cluster_num] = data.loc[data[cluster_col] == cluster_num].fillna(cluster_mode)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # experimenting filling missing values by mode without considering the cluster\n# def fill_nan_by_mode_no_clusters(data, cluster_col):\n#     for col in data.columns:\n#         cluster_mode = data[col].mode()[0]\n#         data[col] = data[col].fillna(cluster_mode)\n#     return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = fill_nan_by_mode(df_copy, 'Clusters')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_copy_3 = fill_nan_by_mode_no_clusters(df_copy_3, 'Clusters')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_copy_3   # this only fill the missing values of each column with its mode, which will cause bias for our model -> the method of filling missing values with mode of the column based on cluster works better","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the number of null values for each column for df_copy2\nfor col in df_copy.columns:\n    print(f\"'{col}' nulls: {df_copy[col].isna().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a lot of columns missing 7427 values, so we suspect that there are 7427 rows that contain missing values in those columns. Observe that there is 'tms_settlement_location' containing 186399 null values which take up a lot of our data, so we might consider dropping that column. After checking the description of this column, the settlement location doesn't really determine the churn behavior of a client (there are features that have stronger relationship to the churn behavior), thus we drop 'tms_settlement_location'. Also, for columns that contain less than 100000 null values, we will drop those null values since they aren't a huge part of our dataset. We perform these methods, then rerun the loop above to check for null values in each column.","metadata":{}},{"cell_type":"code","source":"for col in df_copy.columns:\n    if df_copy[col].isna().sum() < 100000:\n        df_copy.dropna(subset=[col], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = df_copy.drop('tms_settlement_location', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy          # clean df_copy, dataset contains 0 null values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the number of null values for each column for df_copy2\nfor col in df_copy.columns:\n    print(f\"'{col}' nulls: {df_copy[col].isna().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each client has a unique ID, but since the default index column in pandas contains unique indices as well for each record (client), we should drop the 'id' column to reduce the dimension of the encoded dataset later on, as every value in the ID column is unique so there will be many categories when decoding.","metadata":{}},{"cell_type":"code","source":"df_copy = df_copy.drop('id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = df_copy.drop(['plan_effective_date'], axis=1)                                  # drop plan_effective_date, since inception_date dates come first\ndf_copy_time_based_cols = df_copy_time_based_cols.drop(['plan_end_date', 'plan_effective_date'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert date time to just year month day\nfor col in df_copy_time_based_cols.columns:\n    df_copy[col] = pd.to_datetime(df_copy[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the label column to assign to target outcome later on\nlabel_ = df_copy['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy = df_copy.drop(['label', 'Clusters'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the number of categories per column\ntotal_categories = 0\nfor col in df_copy.columns:\n    print(f\"Number of categories in '{col}': {len(df_copy[col].unique())}\")\n    total_categories += len(df_copy[col].unique())\nprint(f\"Total number of categories: {total_categories}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding dataset: Binary encoding\nThere are time-series features that contain a lot of different categories (time, hour). If we chose to perform one-hot encoding, it may cause a very large dataset and there will not be enough memory. Binary Encoding is the alternative.","metadata":{}},{"cell_type":"code","source":"binary_encoder = ce.BinaryEncoder(cols=list(df_copy.columns), return_df=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy_encoded = binary_encoder.fit_transform(df_copy)\ndf_copy_encoded    # encode df_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation between features\ncorr_matrix_ = df_copy_encoded.corr()\ncorr_matrix_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlations between pairs of features that exceed the correlation threshold","metadata":{}},{"cell_type":"code","source":"corr_thresh_ = 0.7\n\ncorr_pairs_ = []\nfor i in range(len(corr_matrix_.columns)):\n    for j in range(i+1, len(corr_matrix_.columns)):\n        if abs(corr_matrix_.iloc[i, j] > corr_thresh_):\n            corr_pairs_.append((corr_matrix_.columns[i], corr_matrix_.columns[j], corr_matrix_.iloc[i, j]))\n\nfor pair in corr_pairs_:\n    print(f\"Features '{pair[0]}' and '{pair[1]}' have correlation of: '{pair[2]}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_thresh_ = 0.7\n\ncorr_pairs_ = []\ncorr_hash = {}\n\nfor i in range(len(corr_matrix_.columns)):\n    for j in range(i+1, len(corr_matrix_.columns)):\n        if abs(corr_matrix_.iloc[i, j] > corr_thresh_):\n            corr_pairs_.append((corr_matrix_.columns[i], corr_matrix_.columns[j], corr_matrix_.iloc[i, j]))\n            # if a feature appears at least 2 times when having high correlation with other features -> drop such features\n            if corr_matrix_.columns[i] in corr_hash:\n                df_copy_encoded = df_copy_encoded.drop(corr_matrix_.columns[i], axis=1)\n                break\n            else:\n                corr_hash[corr_matrix_.columns[i]] = corr_matrix_.iloc[i, j]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the correlation between pair of features after excluding those with high correlation\n\ncorr_thresh_start = 0.5\ncorr_thresh_end = 0.7\n\ncorr_pairs_ = []\nfor i in range(len(corr_matrix_.columns)):\n    for j in range(i+1, len(corr_matrix_.columns)):\n        if abs(corr_matrix_.iloc[i, j] >= corr_thresh_start) and abs(corr_matrix_.iloc[i, j] <= corr_thresh_end):\n            corr_pairs_.append((corr_matrix_.columns[i], corr_matrix_.columns[j], corr_matrix_.iloc[i, j]))\n\nfor pair in corr_pairs_:\n    print(f\"Features '{pair[0]}' and '{pair[1]}' have correlation of: '{pair[2]}'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the list of features which have moderate correlation with other features\nfeat_corr_moderate = []\nfor i in range(len(corr_pairs_)):\n    for j in range(len(corr_pairs_[i][:2])):\n        if corr_pairs_[i][j] not in feat_corr_moderate:\n            feat_corr_moderate.append(corr_pairs_[i][j])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_corr_moderate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Relationship between each feature with moderate correlation and the target outcome","metadata":{}},{"cell_type":"code","source":"df_copy['type_code'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy['number_of_beneficiaries'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MCA: Reduce the dimension for categorical data","metadata":{}},{"cell_type":"code","source":"# init MCA\nmca = prince.MCA(n_components=50, n_iter=10, copy=True, check_input=True, engine='sklearn', random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of components n_components are determined by experimenting how many components are enough to capture a good percentage of variance of our data.","metadata":{}},{"cell_type":"code","source":"# fit MCA to our encoded data\nmca.fit(df_copy_encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # transform df_copy_encoded to the MCA version\n# df_copy_encoded_mca = mca.transform(df_copy_encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the eigenvalues -> see the cummulative variance explained -> pick the number of components that explain enough variance for our data\nmca.eigenvalues_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After trials and failures, 50 components and 10 iterations capture more than 50% of the variance of our encoded data. This implies that 50 is a good number of features that still capture enough variance for our data.","metadata":{}},{"cell_type":"code","source":"# # visualization\n# mca.plot(df_copy_encoded, x_component=0, y_component=1, show_column_markers=True, show_row_markers=True, show_column_labels=False, show_row_labels=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # visualization\n# fig, ax = plt.subplots(figsize=(10, 7))\n# ax.scatter(df_copy_encoded_mca_[0], df_copy_encoded_mca_[1], c='blue', edgecolors='k', alpha=0.5)\n# ax.set_xlabel('Dimension 1')\n# ax.set_ylabel('Dimension 2')\n# ax.set_title('MCA results')\n# plt.axhline(0, color='grey', lw=1)\n# plt.axvline(0, color='grey', lw=1)\n# plt.grid(True)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing data for model training","metadata":{}},{"cell_type":"code","source":"def binary_to_numeric(value):\n    '''\n        Convert \"Churn\" to 1\n        and \"No Churn\" to 0\n    '''\n    if value == \"Churn\":\n        return 1\n    elif value == \"No Churn\":\n        return 0\n    else:\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target outcome\ny_ = label_.apply(binary_to_numeric)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy_encoded['label'] = y_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_copy_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Relationship between features with moderate correlation and the target outcome 'label'","metadata":{}},{"cell_type":"code","source":"# @title\nplt.figure(figsize=(80, 12))  # Adjust the figure size as needed\nsns.set(style=\"whitegrid\")\n\nplt.subplot(2, 2, 1)\nsns.histplot(x=df_copy['type_code'], hue=df_copy_encoded['label'], bins=30, kde=False, color='blue').set(title='Relationship between Outcome and type_code')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recall that label of 1 is Churned, 0 for Not Churned. Looks like most clients with type_code of CASH SWEEP, CASH, and MRGN churned from the service","metadata":{}},{"cell_type":"code","source":"# @title\nplt.figure(figsize=(20, 12))  # Adjust the figure size as needed\nsns.set(style=\"whitegrid\")\n\nplt.subplot(2, 2, 1)\nsns.histplot(x=df_copy['number_of_beneficiaries'], hue=df_copy_encoded['label'], bins=30, kde=False, color='blue').set(title='Relationship between Outcome and number of beneficiaries')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the clients doesnt' have any beneficiaries, and we can see that most clients churned.","metadata":{}},{"cell_type":"code","source":"df_copy_train, df_ = train_test_split(df_copy_encoded, test_size=0.40, random_state=1)\ndf_copy_cv, df_copy_test = train_test_split(df_, test_size=0.50, random_state=1)\n\ndel df_\n\nX_train = df_copy_train.drop('label', axis=1)\ny_train = df_copy_train['label']\n\nX_cv = df_copy_cv.drop('label', axis=1)\ny_cv = df_copy_cv['label']\n\nX_test = df_copy_test.drop('label', axis=1)\ny_test = df_copy_test['label']\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"X_cv shape: {X_cv.shape}\")\nprint(f\"y_cv shape: {y_cv.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.array(X_train).astype(np.float32)\ny_train = np.array(y_train).astype(np.float32)\n\nX_cv = np.array(X_cv).astype(np.float32)\ny_cv = np.array(y_cv).astype(np.float32)\n\nX_test = np.array(X_test).astype(np.float32)\ny_test = np.array(y_test).astype(np.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling:\nAfter getting exhausted with the data processing part, we are now ready to develop a model to discover the underlying relationship and behavior among customers, helping recommend marketing strategies to maximize the likelihood of staying.\n\n#### Motivation\nWe came up with the idea that customers, who may not know each other, will somehow relate to each other based on psychology, for example, and that could determine the churn/no churn behavior of each client. Thus, we treat the customer data as a network, and we will be performing a Stochastic Block Model (SBM) and statistical methods (potentially applying Bernoulli distributions, inhomogeneous Poisson process) to understand the underlying relationship among clients within each clusters, and clients among clusters.","metadata":{}},{"cell_type":"markdown","source":"### XGBoost\nWe start by training our data on a simple XGBoost to predict and classify each client to Churn/No Churn, and use the result to extract the groups clients based on behavior (for example, \"Not Churned, but are likely to churn: At Risk\", \"Churned, but loved a specific product\", \"Not Churned, and enjoy the overall service\").","metadata":{}},{"cell_type":"markdown","source":"### Stochastic Block Model (SBM)\n\n#### Predictive Modeling (for Churn classification and Marketing Strategies to Maximize Likelihood of Staying)\nSBM can predict missing or future connections in networks, and forecast node attributes based on their network position and block memberships.\n\n#### Network Analysis\nIt's a statistical framework used to analyze networks by partitioning nodes (customers) into blocks (clusters) based on theier connectivity (relationship) patterns. It assumes that nodes within the same block have similar interaction probabilities, while interaction probabilities between nodes in different blocks are governed by different parameters. In the context of SBM, the adjacency matrix is analyzed to identify the blocks or segments of customers. These blocks represent groups of customers who interact with each other more frequently than with customers outside their block. The identification of blocks can be achieved through various clustering algorithms that are designed to work with SBM. SBM estimates the probabilities of interactions between and within these blocks\n* A high probability of interaction within a block but low between blocks might indicate a strong community effect that can be significant for marketing strategies\n\n* Notice there are 2 blocks that don't have mutual connection, because the block label is the negation of each other.\n* There are blocks that have connections ('like apples hate grapes' and 'like oranges hate grapes'), because these 2 blocks both hate grapes\n* Generally, the probability of edges (connections) between nodes within the same block is higher than that of between blocks, which makes intuitive sense\n* Each edge represents the likelihood of an edge (connection) existing between pairs of nodes in the network\n* The edge probabilities can vary depending on whether the nodes belong to the same block (within-block edges) or different block (between-block edges)\n\n##### Within-block edge probability\n* Within-block edge probabilities are denoted by p_in\n\n##### Between-block edge probability\n* Between-block edge probabilities are denoted by p_out\n\n##### Probability distribution\n* Edge probabilities in an SBM are modeled using Bernoulli distributions due to the binary nature (either an edge exists or it doesn't)\n* Given a pair of nodes, the probability of an edge existing between them follows a Bernoulli distribution\n\n* p is the probability of success\n* x is the binary outcome variable (0 or 1, representing the absence or presence of an edge)\n* if a random number drawn from this distribution is less than or equal to p (p_in or p_out), an edge is considered to exist between the nodes, otherwise, there is no edge\n\n##### Edge modeling\n* Edge probabilities in SBM are typically estimated from the observed network data using MLE or Bayesian inference\n* These probabilities capture the underlying structure of the network (community structures, connectivity patterns, relationship strengths)","metadata":{}},{"cell_type":"markdown","source":"### Why SBM?\nWhile we were at the stage of processing customer data, we believed that each client within a company, even though they don't know each other, they may likely give off the same churn behavior based on some features on our data.\n\n* Each block is a group of customers with similar behaviors, preferences, or interaction patterns. Customers within the same block are more likely to exhibit similar churn behaviors compared to those in different blocks\n* Customers who are more connected within the network might be less likely to churn due to stronger tie or social influence, word-of-mouth effect, etc.\n* Customers who frequently use complementary products within the same network might exhibit lower churn rates, due to increased product stickiness and statisfaction.","metadata":{}},{"cell_type":"markdown","source":"### Determine the optimal number of blocks (block size), the probability of an edge existing between nodes within each cluster, and the probability of an edge existing between clusters\n* Denote probability of an edge existing between nodes within each cluster: p_in\n* Denote probability of an edge existing between clusters: p_out\n\nTo determine the optimal number of those, we follow the following steps: \n1. Perform PCA for dimensionality reduction over our dataset so that we can extract the most important features\n2. Perform clustering (K-means, K-modes, etc) to cluster the dimensionality reduced dataset\n3. Visualize and get the number of clusters, and use the number for the initial number of blocks of our SBM\n4. Determine the optimal p_in by similarity metric and p_out by dissimilarity metric: Within-cluster similarity can provide insights to strength of connections within blocks. Between-clusters dissimilarity can provide insights to separations between blocks.\n5. We can get the optimal p_in and p_out by MLE by: Calculate the average similarity (edge) within each cluster and assign to p_in. Calculate the average dissimilarity (edge) between clusters and assign to p_out.","metadata":{}},{"cell_type":"markdown","source":"Decoding `investment_objective` data:\n\nSimilar to `risk_tolerance`, for `investment_objective`, the alphanumeric digit indicates the type of security associated with each client, and the number next to each type indicates the probability of having that type of security for each client.","metadata":{"id":"5GuzEBAorPJs"}},{"cell_type":"markdown","source":"#### Determine the feature importance for: Features to determine the connection between blocks, and features to determine the connection between nodes within each block\n* More important features will be used to determine the segmentation of blocks\n* Less important features will be used to determine the segmentation of nodes within each block\n\n#### Strategy development\n* If certain blocks of customers are identified as high risk for churn, specific strategies can be designed for these blocks\n* Understanding the interaction patterns can help in devising referral programs, **cross-selling strategies** and personalized marketing campaigns that leverage the network effects within and across customer blocks","metadata":{}}]}